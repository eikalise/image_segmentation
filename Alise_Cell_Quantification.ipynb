{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+7sXHmIh0q3Zdc7vqtm+O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eikalise/image_segmentation/blob/main/Alise_Cell_Quantification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Required installations"
      ],
      "metadata": {
        "id": "5jPx7Y3UJBfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c-H2a4cOe_j",
        "outputId": "b7c95a56-173f-4aa7-ea8d-bb3d8afb8be4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.2.87)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.19.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.4)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount to Google Drive"
      ],
      "metadata": {
        "id": "glFtu-m0JNv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "#Make sure that you mount to the correct place in google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozNgYBHWNuTM",
        "outputId": "9c0209dc-1b90-4275-d3ec-d69e623bd81a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load and run the model\n",
        "\n",
        "First line of the output will give you how many phase 1s and phase 2s\n",
        "\n",
        "You can look at the prediction photo by clicking on the 4 icon on the left panel (files) --> runs --> detect --> predict#\n"
      ],
      "metadata": {
        "id": "ShlM2cCfJlAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the YOLO model\n",
        "import cv2\n",
        "import time\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# the model (maybe called last.pt or best.pt) must be uploaded to your google drive account\n",
        "# provide the correct path to the model\n",
        "model = YOLO('/content/drive/MyDrive/weights/last.pt')\n",
        "\n",
        "# provide the correct path to the photo\n",
        "photo = cv2.imread('/content/drive/MyDrive/4.jpg') #this is a raw image of the fermentation photo at 50x dilution in RGB\n",
        "\n",
        "# predict # of phase 1 and phase 2 for each photo\n",
        "gray = cv2.cvtColor(photo, cv2.COLOR_BGR2GRAY)\n",
        "clahe = cv2.createCLAHE(clipLimit=16, tileGridSize=(16, 16))\n",
        "processed_photo = clahe.apply(gray)\n",
        "enhanced_3ch = cv2.merge([processed_photo, processed_photo, processed_photo])\n",
        "\n",
        "\n",
        "predict = model.predict(enhanced_3ch, show=True, save=True, save_txt=True, show_boxes= True, show_conf=False, show_labels=False, max_det=350)\n",
        "\n"
      ],
      "metadata": {
        "id": "z3vCJN0MyZyD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7638a3fa-eab4-4409-fa50-0f22b86e9d8f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING ⚠️ Environment does not support cv2.imshow() or PIL Image.show()\n",
            "\n",
            "\n",
            "0: 384x640 48 phase 1s, 190 phase 2s, 368.1ms\n",
            "Speed: 4.9ms preprocess, 368.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Results saved to \u001b[1mruns/detect/predict20\u001b[0m\n",
            "1 label saved to runs/detect/predict20/labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Live Detection by connecting to Microsoft\n",
        "This is unnecessary for now"
      ],
      "metadata": {
        "id": "UyEVlIKdIy1W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8DACX9WkDpk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3c70b79-8f15-4467-caa5-40bbae349172"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/drive/MyDrive/pos_4_fr_1_1.png: 640x640 (no detections), 331.7ms\n",
            "Speed: 4.3ms preprocess, 331.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        }
      ],
      "source": [
        "!pip install msal\n",
        "\n",
        "import msal\n",
        "import requests\n",
        "\n",
        "# Define your Azure app credentials\n",
        "client_id = 'YOUR_CLIENT_ID'\n",
        "client_secret = 'YOUR_CLIENT_SECRET'\n",
        "tenant_id = 'YOUR_TENANT_ID'\n",
        "authority = f'https://login.microsoftonline.com/{tenant_id}'\n",
        "scope = ['https://graph.microsoft.com/.default']\n",
        "\n",
        "# Create a confidential client application\n",
        "app = msal.ConfidentialClientApplication(client_id, authority=authority, client_credential=client_secret)\n",
        "\n",
        "# Acquire a token\n",
        "result = app.acquire_token_for_client(scopes=scope)\n",
        "\n",
        "if 'access_token' in result:\n",
        "    access_token = result['access_token']\n",
        "    print(\"Access token acquired\")\n",
        "else:\n",
        "    print(\"Error acquiring access token\")\n",
        "    print(result)\n",
        "\n",
        "# Define the SharePoint file URL and site details\n",
        "sharepoint_site = 'YOUR_SHAREPOINT_SITE'\n",
        "sharepoint_file_path = 'YOUR_FILE_PATH'\n",
        "sharepoint_drive = 'YOUR_DRIVE_ID'\n",
        "\n",
        "# Create the API URL\n",
        "url = f\"https://graph.microsoft.com/v1.0/sites/{sharepoint_site}/drives/{sharepoint_drive}/root:/{sharepoint_file_path}:/content\"\n",
        "\n",
        "# Make the request to download the file\n",
        "headers = {'Authorization': f'Bearer {access_token}'}\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    with open('downloaded_image.jpg', 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    print(\"Image downloaded successfully\")\n",
        "else:\n",
        "    print(\"Failed to download image\")\n",
        "    print(response.json())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Live Detection by connecting camera usb to computer\n",
        "Doesn't work on goodle colab -- needs Jupyter notebook or other alternatives"
      ],
      "metadata": {
        "id": "aZDoWlwLQQAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import time\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "\n",
        "def apply_clahe(image, clip_limit=16.0, tile_grid_size=(8, 8)):\n",
        "\n",
        "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
        "    return clahe.apply(image)\n",
        "\n",
        "def live_inference_with_yolo_and_clahe(model, clip_limit=2.0, tile_grid_size=(8, 8), conf_threshold=0.05):\n",
        "    \"\"\"\n",
        "    Perform live inference using a YOLO model from a camera feed with CLAHE enhancement on every frame.\n",
        "\n",
        "    Parameters:\n",
        "    model (YOLO): The loaded YOLO model.\n",
        "    clip_limit (float): Threshold for contrast limiting.\n",
        "    tile_grid_size (tuple): Size of the grid for the histogram equalization.\n",
        "    conf_threshold (float): Confidence threshold for predictions.\n",
        "    \"\"\"\n",
        "    # Initialize the camera\n",
        "    camera = cv2.VideoCapture(0)  # Use '0' for the default camera, replace with camera index if necessary\n",
        "\n",
        "    if not camera.isOpened():\n",
        "        print(\"Error: Could not open camera.\")\n",
        "        return\n",
        "\n",
        "    while True:\n",
        "        # Capture frame-by-frame\n",
        "        ret, frame = camera.read()\n",
        "        if not ret:\n",
        "            print(\"Failed to grab frame\")\n",
        "            break\n",
        "\n",
        "        # Convert the frame to grayscale\n",
        "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Apply CLAHE\n",
        "        enhanced_frame = apply_clahe(gray_frame, clip_limit=clip_limit, tile_grid_size=tile_grid_size)\n",
        "\n",
        "        # Stack the grayscale image to create a 3-channel image\n",
        "        enhanced_frame_3ch = cv2.merge([enhanced_frame, enhanced_frame, enhanced_frame])\n",
        "\n",
        "        # Run YOLOv8 inference on the enhanced frame\n",
        "        results = model.predict(enhanced_frame_3ch, conf=conf_threshold)\n",
        "\n",
        "        # Draw bounding boxes and labels on the enhanced grayscale image\n",
        "        if results:\n",
        "            for result in results[0].boxes:\n",
        "                # Get bounding box coordinates\n",
        "                x1, y1, x2, y2 = map(int, result.xyxy[0])\n",
        "\n",
        "                # Draw the bounding box\n",
        "                cv2.rectangle(enhanced_frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "\n",
        "                # Draw the label\n",
        "                label = f\"{int(result.cls.item())} {result.conf.item():.2f}\"\n",
        "                cv2.putText(enhanced_frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "\n",
        "        # Display the resulting frame\n",
        "        cv2.imshow('YOLOv8 Live Predictions with CLAHE', enhanced_frame)\n",
        "\n",
        "        # Break the loop on 'q' key press\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    # Release the camera and close windows\n",
        "    camera.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Specify the model path\n",
        "model_path = '/content/drive/MyDrive/last.pt'  # Replace with your model's path\n",
        "\n",
        "# Load the YOLO model\n",
        "model = YOLO(model_path)\n",
        "\n",
        "# Perform live inference with YOLO and CLAHE enhancement\n",
        "live_inference_with_yolo_and_clahe(model, clip_limit=16.0, tile_grid_size=(8, 8), conf_threshold=0.05)\n"
      ],
      "metadata": {
        "id": "evqr_DsPO_Pw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "714675f0-a7d0-464d-9246-e16d62c77206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Could not open camera.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2LYCQ795n9en"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}